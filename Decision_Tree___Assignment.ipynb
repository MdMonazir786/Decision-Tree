{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree:\n",
        "1. What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "Ans:- A Decision Tree is a supervised learning model that predicts a target by learning simple decision rules from data, organized in a tree‑like structure of nodes and branches.\n",
        "\n",
        "What a decision tree is:\n",
        "\n",
        "a. It consists of:\n",
        "\n",
        "(i). Root node: represents the full dataset and the first splitting feature.\n",
        "\n",
        "(ii). Internal nodes: decision tests on feature values (for example, “Age > 30?”).\n",
        "\n",
        "(iii). Branches: outcomes of those tests (Yes/No or categorical values).\n",
        "\n",
        "(iv). Leaf nodes: final class labels or predictions.\n",
        "\n",
        "b. It is widely used for classification (categorical outputs) and also for regression (continuous outputs).\n",
        "\n",
        "How it works for classification:\n",
        "\n",
        "a. Starting at the root, the algorithm recursively splits the data based on feature values to create subsets that are as “pure” as possible (mostly containing one class).\n",
        "\n",
        "b. At each node it chooses the best feature and threshold using a impurity criterion such as:\n",
        "\n",
        "(i). Gini impurity (used in CART),\n",
        "\n",
        "(ii). Entropy / information gain (used in ID3/C4.5).\n",
        "\n",
        "c. This process continues until a stopping rule is met (all samples in a node share the same class, depth limit reached, or not enough samples). The node then becomes a leaf with a class label (often the majority class in that node).\n",
        "\n",
        "\n",
        "Making a classification prediction:\n",
        "\n",
        "For a new example:\n",
        "\n",
        "a. Start at the root, apply the node's test (e.g., “Income > 50k?”).\n",
        "\n",
        "b. Follow the corresponding branch (Yes/No or specific category).\n",
        "\n",
        "c. Repeat at each internal node until reaching a leaf.\n",
        "\n",
        "d. The class assigned to that leaf is the predicted class for that example.\n",
        "\n",
        "2. Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "\n",
        "Ans:- Gini impurity and entropy are node impurity measures used by decision trees to decide how to split the data. Both quantify how “mixed” the class labels are at a node: lower value = purer node = better split candidate.\n",
        "\n",
        "Gini impurity:\n",
        "\n",
        "a. For a node with class probabilities p1,p2,…,pK, Gini impurity is,\n",
        "\n",
        "Gini = 1 - Σ(pᵢ²)\n",
        "\n",
        "b. Interpretation: approximate probability that a randomly chosen sample in the node would be misclassified if you randomly assigned labels according to the node's class distribution.\n",
        "\n",
        "c. Properties:\n",
        "\n",
        "(i). G=0 when all samples belong to a single class (perfectly pure).\n",
        "\n",
        "(ii). Larger when classes are more evenly mixed (maximum for balanced classes, e.g., 0.5 for a 50-50 binary node).\n",
        "\n",
        "Entropy:\n",
        "\n",
        "a. For the same node, entropy (from information theory) is,\n",
        "\n",
        "H = -Σ p(x) log₂(p(x))\n",
        "\n",
        "b. Interpretation: expected number of bits needed to encode the class label; higher entropy = more disorder or uncertainty.\n",
        "\n",
        "c. Properties:\n",
        "\n",
        "(i). H=0 when the node is pure (all in one class).\n",
        "\n",
        "(ii). Maximum when all classes are equally likely (most uncertain / mixed).\n",
        "\n",
        "How they affect splits in a decision tree:\n",
        "\n",
        "A. During training, a decision tree evaluates many possible splits at a node:\n",
        "\n",
        "a. For each candidate split, it computes the weighted impurity of the child nodes (using either Gini or entropy).\n",
        "\n",
        "b. It then chooses the split that maximizes impurity reduction (equivalently, minimizes the weighted impurity of children):\n",
        "\n",
        "(i). For Gini: sometimes called Gini gain.\n",
        "\n",
        "(ii). For entropy: called information gain.\n",
        "\n",
        "B. Impact:\n",
        "\n",
        "a. Both criteria prefer splits that create child nodes where one class dominates (high purity).\n",
        "\n",
        "b. In practice they often pick very similar splits; Gini is slightly more sensitive to the majority class and is a bit faster to compute (no logarithms).\n",
        "\n",
        "c. Using either measure biases the tree toward features and thresholds that most reduce class mixing early in the tree, which usually improves classification accuracy and makes decision boundaries sharper.\n",
        "\n",
        "3. What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "\n",
        "Ans:- Pre-pruning and post-pruning are two ways of simplifying a decision tree to reduce overfitting, but they act at different stages of tree construction and have different practical benefits.\n",
        "\n",
        "Pre-pruning (early stopping):\n",
        "\n",
        "a. Concept: The tree is stopped from growing while it is being built if a split fails some heuristic test, such as “information gain below a threshold,” “node has fewer than min_samples_split,” or “depth reached max_depth.”\n",
        "\n",
        "b. Goal: Prevent the tree from becoming too deep or complex in the first place, trading some bias for lower variance and faster training.\n",
        "\n",
        "Practical advantage: Pre-pruning is computationally efficient—it avoids building unnecessary branches, which is valuable on large datasets or when models must be trained frequently or in real time.\n",
        "\n",
        "Post-pruning (pruning after full growth):\n",
        "\n",
        "a. Concept: First grow a large (often overfitted) tree with little or no stopping, then prune it back by cutting subtrees whose removal does not hurt—or even improves—performance on validation data or according to a cost-complexity criterion.\n",
        "\n",
        "b. Methods: Examples include cost-complexity (minimal cost-complexity) pruning, reduced-error pruning, and other subtree-replacement strategies.\n",
        "\n",
        "Practical advantage: Post-pruning often yields a better-performing and more robust tree, because it makes pruning decisions with knowledge of the entire grown tree and its behavior on validation data, rather than relying only on local heuristics while growing.\n",
        "\n",
        "Impact on trees:\n",
        "\n",
        "Pre-pruning: stops growth based on local criteria → smaller, faster-to-train trees, but risk of underfitting if stopped too early.\n",
        "\n",
        "Post-pruning: lets tree overfit, then trims back using validation or complexity penalties → more computation, but usually better generalization and cleaner structure.\n",
        "\n",
        "4. What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "\n",
        "Ans:- Information Gain measures how much a split reduces uncertainty (entropy) about the class labels at a node, and the tree chooses the feature/threshold with the highest Information Gain as the best split.\n",
        "\n",
        "Definition:\n",
        "\n",
        "a. Entropy at a node with class probabilities p1,…,pK:\n",
        "\n",
        "H(parent) = -∑ pi.log2.pi\n",
        "\n",
        "b. After splitting on attribute a into child nodes j, each with proportion\n",
        "(|Tj|/|T|) of samples and entropy H(Tj), the Information Gain of that split is:\n",
        "\n",
        "IG(T,a) = H(T)-∑|Tj|/|T|*H(TJ)\n",
        "\n",
        "This is the reduction in entropy from the parent node to the children.\n",
        "\n",
        "Why it matters for choosing the best split:\n",
        "\n",
        "a. Selects purer children: A high Information Gain means the child nodes have much lower entropy, i.e., they are more homogeneous in class labels, so classification there is easier and more accurate.\n",
        "\n",
        "b. Feature ranking: At each node, the algorithm computes Information Gain for all candidate features (and thresholds for numeric features) and chooses the split with the largest gain, building the tree top-down.\n",
        "\n",
        "c. Greedy but effective: This greedy choice of maximum Information Gain at each step tends to create short, informative paths from root to leaf, improving both accuracy and interpretability of the resulting tree.\n",
        "\n",
        "5. What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "Ans:- Decision Trees are widely used because they combine reasonable predictive power with strong interpretability, but they also have well-known weaknesses like overfitting and instability.\n",
        "\n",
        "Common real-world applications:\n",
        "\n",
        "a. Credit scoring and loan approval (banking):\n",
        "\n",
        "Assess whether to approve a loan based on features like credit score, income, employment stability, and past defaults.\n",
        "\n",
        "b. Medical diagnosis and treatment support (healthcare):\n",
        "\n",
        "Classify patients into disease/no-disease groups (e.g., diabetes, heart disease) using lab values and symptoms, or select treatment paths.\n",
        "\n",
        "c. Customer churn and marketing segmentation:\n",
        "\n",
        "Predict which customers are likely to churn or respond to an offer, using demographic and behavior features, then design targeted campaigns.\n",
        "\n",
        "d. Fraud detection (finance, insurance, e-commerce):\n",
        "\n",
        "Flag suspicious transactions or claims based on patterns in historical fraud vs. non-fraud records.\n",
        "\n",
        "e. Manufacturing and quality control:\n",
        "\n",
        "Identify process conditions that lead to defects or failures, so that operators can adjust parameters and improve yield.\n",
        "\n",
        "f. perations and strategy / decision support:\n",
        "\n",
        "Evaluate business or project decisions (e.g., expansion options, pricing, logistics) by mapping scenarios, costs, and outcomes in tree form.\n",
        "\n",
        "Main advantages:\n",
        "\n",
        "a. Highly interpretable:\n",
        "\n",
        "Trees read like nested if-else rules and can be visualized, making them easy for non-experts and stakeholders to understand and audit.\n",
        "\n",
        "b. Handle different data types and non-linear patterns:\n",
        "\n",
        "Work with both numerical and categorical features and naturally model non-linear decision boundaries via hierarchical splits.\n",
        "\n",
        "c. Require little preprocessing:\n",
        "\n",
        "No need for feature scaling; trees can automatically pick informative features and ignore many irrelevant ones.\n",
        "\n",
        "d. Versatile:\n",
        "\n",
        "Support both classification and regression, and serve as base learners in powerful ensembles like Random Forests and Gradient Boosted Trees.\n",
        "\n",
        "Main limitations:\n",
        "\n",
        "a. Overfitting and high variance:\n",
        "\n",
        "Deep trees can memorize training data and perform poorly on unseen data; small changes in data can lead to very different tree structures.\n",
        "\n",
        "b. Instability and sensitivity to noise:\n",
        "\n",
        "Because each split decision is greedy, noise or outliers can heavily influence structure and predictions.\n",
        "\n",
        "c. Bias toward features with many categories & imbalanced classes:\n",
        "\n",
        "Standard splitting criteria may favor attributes with many distinct values and can be biased toward majority classes if data are imbalanced.\n",
        "\n",
        "d. Often outperformed by ensembles or other models:\n",
        "\n",
        "Single trees are usually less accurate than ensemble methods (Random Forest, XGBoost) or complex models like neural networks, especially for regression or high-dimensional data."
      ],
      "metadata": {
        "id": "p1GveNi17J9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 Write a Python program to:\n",
        "# Load the Iris Dataset\n",
        "# Train a Decision Tree Classifier using the Gini criterion\n",
        "# Print the model’s accuracy and feature importances\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()                    # features in iris.data, labels in iris.target\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train a Decision Tree Classifier using the Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Compute accuracy on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.3f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature importances:\")\n",
        "for name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {importance:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BQMzVJ5I0SM",
        "outputId": "f3f7f16b-82c2-4545-b889-63108383b23f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.933\n",
            "Feature importances:\n",
            "sepal length (cm): 0.006\n",
            "sepal width (cm): 0.029\n",
            "petal length (cm): 0.559\n",
            "petal width (cm): 0.406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7 Write a Python program to:\n",
        "# Load the Iris Dataset\n",
        "# Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Decision Tree with max_depth = 3 (pre-pruned tree)\n",
        "tree_depth3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree_depth3.fit(X_train, y_train)\n",
        "y_pred_3 = tree_depth3.predict(X_test)\n",
        "acc_3 = accuracy_score(y_test, y_pred_3)\n",
        "\n",
        "# Fully-grown Decision Tree (no depth limit)\n",
        "tree_full = DecisionTreeClassifier(random_state=42)   # max_depth=None by default\n",
        "tree_full.fit(X_train, y_train)\n",
        "y_pred_full = tree_full.predict(X_test)\n",
        "acc_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "print(f\"Accuracy with max_depth=3: {acc_3:.3f}\")\n",
        "print(f\"Accuracy with full tree : {acc_full:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxJLr_a-LjII",
        "outputId": "5e813e41-444e-4317-8d6c-f6a8a80ae2ce"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 0.967\n",
            "Accuracy with full tree : 0.933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Write a Python program to:\n",
        "# Load the California Housing dataset from sklearn\n",
        "# Train a Decision Tree Regressor\n",
        "# Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "dt = DecisionTreeRegressor(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate MSE\n",
        "y_pred = dt.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Feature importances:\", dt.feature_importances_)\n",
        "\n"
      ],
      "metadata": {
        "id": "kEUhlHuWMKrz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Write a Python program to:\n",
        "# Load the Iris Dataset\n",
        "# Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
        "# Print the best parameters and the resulting model accuracy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Set up the Decision Tree and parameter grid\n",
        "dt = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "\n",
        "param_grid = {\"max_depth\": [2, 3, 4, 5, None], \"min_samples_split\": [2, 5, 10]}\n",
        "\n",
        "# GridSearchCV to tune hyperparameters (5-fold CV)\n",
        "grid = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring=\"accuracy\")\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate best model on the test set\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best parameters:\", grid.best_params_)\n",
        "print(f\"Cross‑val best accuracy: {grid.best_score_:.3f}\")\n",
        "print(f\"Test accuracy with best params: {test_acc:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq3gvoGLNYFk",
        "outputId": "67e8b500-11ce-45f0-e816-2ca30e0023ce"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Cross‑val best accuracy: 0.942\n",
            "Test accuracy with best params: 0.933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  Imagine you're working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "Ans:- Concise step-by-step process we could describe as.\n",
        "\n",
        "A. Handle the missing values:\n",
        "\n",
        "a. Explore missingness:\n",
        "\n",
        "Check percentage of missing values per column and whether it’s MCAR/MAR/MNAR (completely at random, at random, not at random).\n",
        "\n",
        "b. Impute appropriately:\n",
        "\n",
        "(i). For numerical features: use median (robust to outliers) or mean; consider more advanced imputation (KNN/iterative) if many values are missing.\n",
        "\n",
        "(ii). For categorical features: impute with most frequent category or add a special “Unknown” category.\n",
        "\n",
        "c. Use a pipeline so imputation is part of the training process and applied consistently to train and test sets.\n",
        "\n",
        "B. Encode the categorical features:\n",
        "\n",
        "a. Because Decision Trees can naturally handle ordinal and integer-coded categories, but many libraries expect numbers, encode as:\n",
        "\n",
        "(i). One-hot encoding for nominal variables (e.g., gender, blood type).\n",
        "\n",
        "(ii). Ordinal encoding only when there is a genuine order (e.g., disease stage I < II < III).\n",
        "\n",
        "b. Again, put encoders in a ColumnTransformer / pipeline so the transformations are learned only on training data and applied to validation/test data.\n",
        "\n",
        "C. Train a Decision Tree model:\n",
        "\n",
        "a. Split data into train/validation/test (or train/test with cross-validation).\n",
        "\n",
        "b. Create a DecisionTreeClassifier with a reasonable starting configuration (e.g., criterion=\"gini\", limited max_depth to avoid severe overfitting).\n",
        "\n",
        "c. Fit the model on the processed training data.\n",
        "\n",
        "D. Tune its hyperparameters:\n",
        "\n",
        "a. Use GridSearchCV or RandomizedSearchCV over parameters such as:\n",
        "\n",
        "(i). max_depth (tree depth)\n",
        "\n",
        "(ii). min_samples_split, min_samples_leaf (minimum samples to form splits / leaves)\n",
        "\n",
        "(iii). max_features (number of features considered per split)\n",
        "\n",
        "b. Optimize for an appropriate metric (e.g., ROC-AUC, F1-score) using cross-validation to get a robust estimate.\n",
        "\n",
        "c. Select the best model and refit it on the full training set.\n",
        "\n",
        "E. Evaluate its performance:\n",
        "\n",
        "a. On the held-out test set, compute:\n",
        "\n",
        "(i). Confusion matrix, accuracy\n",
        "\n",
        "(ii). Precision, recall, F1-score (especially important if the disease is rare)\n",
        "\n",
        "(iii). ROC curve and AUC to understand discrimination across thresholds.\n",
        "\n",
        "F. Business value in a real-world healthcare setting:\n",
        "\n",
        "(i). Earlier, data-driven triage: High-risk patients can be flagged for additional tests or specialist review, potentially catching disease earlier and improving outcomes.\n",
        "\n",
        "(ii). Resource optimization: Hospitals can prioritize expensive diagnostics for patients with the highest predicted risk, reducing unnecessary tests and costs.\n",
        "\n",
        "(iii). Clinical decision support: Because Decision Trees are interpretable (if not too deep), clinicians can see which factors led to a high-risk prediction, supporting transparent, explainable decisions.\n",
        "\n",
        "(iv). Population health management: Aggregated predictions help identify high-risk cohorts, guiding preventive programs and targeted interventions (screening campaigns, lifestyle counseling).\n"
      ],
      "metadata": {
        "id": "VZgrXnQUPn67"
      }
    }
  ]
}